{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe73ab3",
   "metadata": {},
   "source": [
    "# ğŸš€ **Ready for Google Colab!** \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JishnuPG-tech/neural-network-appliance-energy-prediction/blob/main/notebooks/01_data_exploration.ipynb)\n",
    "\n",
    "**Click the badge above to open this notebook in Google Colab instantly!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821effb1",
   "metadata": {},
   "source": [
    "# ğŸ” Neural Network Appliance Energy Prediction - Data Exploration\n",
    "\n",
    "**Advanced Deep Learning Project for Individual Appliance Energy Consumption Prediction**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JishnuPG-tech/neural-network-appliance-energy-prediction/blob/main/notebooks/01_data_exploration.ipynb)\n",
    "\n",
    "## ğŸ“‹ Project Overview\n",
    "\n",
    "This notebook explores appliance energy consumption data for building a neural network model. We'll analyze patterns, relationships, and prepare insights for our TensorFlow/Keras deep learning model.\n",
    "\n",
    "### ğŸ¯ Objectives:\n",
    "- **Data Understanding**: Explore appliance energy consumption patterns\n",
    "- **Feature Analysis**: Identify key features for neural network training\n",
    "- **Visualization**: Create comprehensive charts and plots\n",
    "- **Statistical Analysis**: Understand data distributions and correlations\n",
    "- **Neural Network Preparation**: Prepare data insights for deep learning\n",
    "\n",
    "### \udd27 Technology Stack:\n",
    "- **Deep Learning**: TensorFlow/Keras for neural networks\n",
    "- **Data Analysis**: Pandas, NumPy for data manipulation\n",
    "- **Visualization**: Matplotlib, Seaborn, Plotly for charts\n",
    "- **Machine Learning**: Scikit-learn for preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e973718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Install and Import Required Libraries for Neural Network Analysis\n",
    "\n",
    "# For Google Colab: Install additional packages if needed\n",
    "try:\n",
    "    import google.colab\n",
    "    # Install packages in Colab environment\n",
    "    !pip install -q plotly\n",
    "    !pip install -q seaborn\n",
    "    print(\"ğŸ”§ Running in Google Colab - packages installed!\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running in local environment\")\n",
    "\n",
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries  \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "from scipy import stats\n",
    "\n",
    "# System libraries\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸ§  Ready for Neural Network Data Exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8876c",
   "metadata": {},
   "source": [
    "## \udcca Load and Examine the Dataset\n",
    "\n",
    "For this neural network project, we'll work with comprehensive appliance energy consumption data. The dataset includes multiple features that will be used to train our deep learning model.\n",
    "\n",
    "### ğŸ” Data Source Options:\n",
    "1. **Local Data**: Load from data/sample/ directory\n",
    "2. **Generated Data**: Create sample data for demonstration\n",
    "3. **External Data**: Download from online sources (for Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ Load Dataset for Neural Network Analysis\n",
    "\n",
    "# For Google Colab: Clone repository or create sample data\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ğŸŒ Running in Google Colab\")\n",
    "    \n",
    "    # Option 1: Clone the repository\n",
    "    !git clone https://github.com/JishnuPG-tech/neural-network-appliance-energy-prediction.git\n",
    "    os.chdir('/content/neural-network-appliance-energy-prediction')\n",
    "    data_path = 'data/sample/appliance_energy_dataset.csv'\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ğŸ’» Running in local environment\")\n",
    "    data_path = '../data/sample/appliance_energy_dataset.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Sample data not found. Generating demo data...\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'appliance_type': np.random.choice(['refrigerator', 'air_conditioner', 'washing_machine', 'dishwasher', 'television'], n_samples),\n",
    "        'power_rating': np.random.randint(100, 2000, n_samples),\n",
    "        'daily_hours': np.random.uniform(1, 24, n_samples),\n",
    "        'room_size': np.random.randint(10, 50, n_samples),\n",
    "        'household_size': np.random.randint(1, 6, n_samples),\n",
    "        'season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], n_samples),\n",
    "        'daily_consumption': np.random.uniform(0.5, 15.0, n_samples),\n",
    "        'efficiency_score': np.random.randint(1, 5, n_samples)\n",
    "    })\n",
    "    print(\"âœ… Demo dataset created successfully!\")\n",
    "\n",
    "print(f\"\\nğŸ” Dataset Overview:\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f68c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‹ Dataset Information and Structure\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\udcca NEURAL NETWORK DATASET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic information\n",
    "print(f\"\\nğŸ”¢ Dataset Dimensions:\")\n",
    "print(f\"   Samples: {df.shape[0]:,}\")\n",
    "print(f\"   Features: {df.shape[1]}\")\n",
    "\n",
    "# Data types and info\n",
    "print(f\"\\nğŸ“ Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nğŸ‘€ First 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e2b567",
   "metadata": {},
   "source": [
    "## 2. ğŸ” Understanding Our Key Variables\n",
    "\n",
    "Let's examine the most important variables that our neural network will use to predict appliance energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” Data Quality Assessment for Neural Network Training\n",
    "\n",
    "print(\"ğŸ” DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "print(f\"\\nâŒ Missing Values:\")\n",
    "if missing_data.sum() == 0:\n",
    "    print(\"   âœ… No missing values found!\")\n",
    "else:\n",
    "    for col, missing in missing_data.items():\n",
    "        if missing > 0:\n",
    "            print(f\"   {col}: {missing} ({missing_percent[col]:.2f}%)\")\n",
    "\n",
    "# Duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nğŸ”„ Duplicate Rows: {duplicates}\")\n",
    "\n",
    "# Data types suitable for neural networks\n",
    "print(f\"\\nğŸ§  Neural Network Compatibility:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"   Numeric features: {len(numeric_cols)} âœ…\")\n",
    "print(f\"   Categorical features: {len(categorical_cols)} (need encoding)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Categories:\")\n",
    "print(f\"   Numeric: {numeric_cols}\")\n",
    "print(f\"   Categorical: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Target Variable Analysis for Neural Network Prediction\n",
    "\n",
    "# Identify target variable (daily_consumption for energy prediction)\n",
    "target_col = 'daily_consumption'\n",
    "\n",
    "if target_col in df.columns:\n",
    "    print(f\"\udfaf TARGET VARIABLE ANALYSIS: {target_col}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    target_stats = df[target_col].describe()\n",
    "    print(f\"\\nğŸ“ˆ Statistical Summary:\")\n",
    "    for stat, value in target_stats.items():\n",
    "        print(f\"   {stat.capitalize()}: {value:.3f}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    print(f\"\\n\udcca Distribution Analysis:\")\n",
    "    print(f\"   Range: {df[target_col].min():.3f} - {df[target_col].max():.3f}\")\n",
    "    print(f\"   Variance: {df[target_col].var():.3f}\")\n",
    "    print(f\"   Skewness: {df[target_col].skew():.3f}\")\n",
    "    print(f\"   Kurtosis: {df[target_col].kurtosis():.3f}\")\n",
    "    \n",
    "    # Check for outliers using IQR method\n",
    "    Q1 = df[target_col].quantile(0.25)\n",
    "    Q3 = df[target_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[target_col] < lower_bound) | (df[target_col] > upper_bound)]\n",
    "    \n",
    "    print(f\"\\nğŸš¨ Outlier Analysis:\")\n",
    "    print(f\"   Outliers detected: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    print(f\"   Lower bound: {lower_bound:.3f}\")\n",
    "    print(f\"   Upper bound: {upper_bound:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš ï¸  Target column '{target_col}' not found in dataset\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de833bc2",
   "metadata": {},
   "source": [
    "## 3. ğŸ“Š Visualizing Appliance Energy Consumption Patterns\n",
    "\n",
    "Now let's create beautiful visualizations to understand how different appliances consume energy. This will help us understand what patterns our neural network needs to learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de312a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Comprehensive Data Visualization for Neural Network Insights\n",
    "\n",
    "print(\"ğŸ¨ CREATING NEURAL NETWORK DATA VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸ§  Neural Network Data Exploration Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Target Variable Distribution\n",
    "if 'daily_consumption' in df.columns:\n",
    "    axes[0, 0].hist(df['daily_consumption'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('\udcc8 Daily Energy Consumption Distribution')\n",
    "    axes[0, 0].set_xlabel('Daily Consumption (kWh)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Appliance Type Distribution\n",
    "if 'appliance_type' in df.columns:\n",
    "    appliance_counts = df['appliance_type'].value_counts()\n",
    "    axes[0, 1].bar(appliance_counts.index, appliance_counts.values, color='lightcoral', alpha=0.8)\n",
    "    axes[0, 1].set_title('ğŸ”Œ Appliance Type Distribution')\n",
    "    axes[0, 1].set_xlabel('Appliance Type')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Power Rating vs Daily Consumption\n",
    "if 'power_rating' in df.columns and 'daily_consumption' in df.columns:\n",
    "    axes[1, 0].scatter(df['power_rating'], df['daily_consumption'], alpha=0.6, color='green')\n",
    "    axes[1, 0].set_title('âš¡ Power Rating vs Daily Consumption')\n",
    "    axes[1, 0].set_xlabel('Power Rating (Watts)')\n",
    "    axes[1, 0].set_ylabel('Daily Consumption (kWh)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Seasonal Energy Consumption\n",
    "if 'season' in df.columns and 'daily_consumption' in df.columns:\n",
    "    seasonal_avg = df.groupby('season')['daily_consumption'].mean()\n",
    "    axes[1, 1].bar(seasonal_avg.index, seasonal_avg.values, color='orange', alpha=0.8)\n",
    "    axes[1, 1].set_title('\udf0d Seasonal Energy Consumption')\n",
    "    axes[1, 1].set_xlabel('Season')\n",
    "    axes[1, 1].set_ylabel('Average Daily Consumption (kWh)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Neural Network data visualizations created successfully!\")\n",
    "print(\"\udcca These insights will guide our deep learning model architecture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04b87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”— Correlation Analysis for Neural Network Feature Selection\n",
    "\n",
    "print(\"ğŸ”— CORRELATION ANALYSIS FOR NEURAL NETWORK\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Select numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "if not numeric_df.empty:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    \n",
    "    print(f\"ğŸ“Š Correlation Matrix ({numeric_df.shape[1]} features):\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('ğŸ§  Neural Network Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Features')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify highly correlated features (important for neural networks)\n",
    "    if 'daily_consumption' in correlation_matrix.columns:\n",
    "        target_corr = correlation_matrix['daily_consumption'].abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Feature Importance for Neural Network (correlation with target):\")\n",
    "        print(\"=\" * 65)\n",
    "        for feature, corr in target_corr.items():\n",
    "            if feature != 'daily_consumption':\n",
    "                importance = \"ğŸ”¥ High\" if corr > 0.5 else \"ğŸ“Š Medium\" if corr > 0.3 else \"ğŸ“‰ Low\"\n",
    "                print(f\"   {feature}: {corr:.3f} {importance}\")\n",
    "        \n",
    "        print(f\"\\nğŸ§  Neural Network Feature Selection Insights:\")\n",
    "        strong_features = target_corr[target_corr > 0.5].index.tolist()\n",
    "        if 'daily_consumption' in strong_features:\n",
    "            strong_features.remove('daily_consumption')\n",
    "        \n",
    "        print(f\"   Strong predictors ({len(strong_features)}): {strong_features}\")\n",
    "        print(f\"   These features will be crucial for neural network training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  No numeric features found for correlation analysis.\")\n",
    "\n",
    "print(f\"\\nâœ… Correlation analysis completed for neural network optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32ea7b",
   "metadata": {},
   "source": [
    "## 4. ğŸ”— Correlation Analysis - Finding Relationships\n",
    "\n",
    "Understanding how different variables relate to energy consumption is crucial for our neural network. Let's explore these relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation analysis\n",
    "numerical_cols = ['daily_consumption_kwh', 'power_rating_watts', 'usage_hours_per_day',\n",
    "                 'efficiency_rating', 'appliance_age_years', 'household_size', \n",
    "                 'monthly_cost_inr']\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Create a beautiful heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt='.3f',\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "\n",
    "plt.title('ğŸ”— Correlation Matrix - Understanding Variable Relationships', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the strongest correlations with daily consumption\n",
    "consumption_correlations = correlation_matrix['daily_consumption_kwh'].abs().sort_values(ascending=False)\n",
    "print(\"ğŸ¯ STRONGEST CORRELATIONS WITH DAILY CONSUMPTION:\")\n",
    "print(\"=\" * 55)\n",
    "for variable, correlation in consumption_correlations.items():\n",
    "    if variable != 'daily_consumption_kwh':\n",
    "        strength = \"Very Strong\" if correlation > 0.7 else \"Strong\" if correlation > 0.5 else \"Moderate\" if correlation > 0.3 else \"Weak\"\n",
    "        print(f\"ğŸ“Š {variable}: {correlation:.3f} ({strength})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503accf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of key relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Power Rating vs Consumption\n",
    "axes[0, 0].scatter(df['power_rating_watts'], df['daily_consumption_kwh'], \n",
    "                   alpha=0.6, color='blue', s=50)\n",
    "axes[0, 0].set_xlabel('Power Rating (Watts)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Daily Consumption (kWh)', fontweight='bold')\n",
    "axes[0, 0].set_title('âš¡ Power Rating vs Daily Consumption', fontweight='bold')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['power_rating_watts'], df['daily_consumption_kwh'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 0].plot(df['power_rating_watts'], p(df['power_rating_watts']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Plot 2: Usage Hours vs Consumption\n",
    "axes[0, 1].scatter(df['usage_hours_per_day'], df['daily_consumption_kwh'], \n",
    "                   alpha=0.6, color='green', s=50)\n",
    "axes[0, 1].set_xlabel('Usage Hours per Day', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Daily Consumption (kWh)', fontweight='bold')\n",
    "axes[0, 1].set_title('â° Usage Hours vs Daily Consumption', fontweight='bold')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['usage_hours_per_day'], df['daily_consumption_kwh'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0, 1].plot(df['usage_hours_per_day'], p(df['usage_hours_per_day']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Plot 3: Efficiency Rating vs Consumption\n",
    "axes[1, 0].scatter(df['efficiency_rating'], df['daily_consumption_kwh'], \n",
    "                   alpha=0.6, color='orange', s=50)\n",
    "axes[1, 0].set_xlabel('Efficiency Rating (1-5)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Daily Consumption (kWh)', fontweight='bold')\n",
    "axes[1, 0].set_title('â­ Efficiency Rating vs Daily Consumption', fontweight='bold')\n",
    "\n",
    "# Plot 4: Age vs Consumption\n",
    "axes[1, 1].scatter(df['appliance_age_years'], df['daily_consumption_kwh'], \n",
    "                   alpha=0.6, color='red', s=50)\n",
    "axes[1, 1].set_xlabel('Appliance Age (Years)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Daily Consumption (kWh)', fontweight='bold')\n",
    "axes[1, 1].set_title('ğŸ“… Appliance Age vs Daily Consumption', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“‹ RELATIONSHIP ANALYSIS SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"ğŸ”‹ POWER RATING: Higher power rating = Higher consumption (Expected!)\")\n",
    "print(\"â±ï¸  USAGE HOURS: More usage hours = Higher consumption (Makes sense!)\")\n",
    "print(\"â­ EFFICIENCY: Higher efficiency = Lower consumption (Great for savings!)\")\n",
    "print(\"ğŸ“† AGE: Older appliances tend to consume more (Efficiency degrades over time)\")\n",
    "print(\"\\nğŸ’¡ These patterns will help our neural network learn to predict consumption!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ca865",
   "metadata": {},
   "source": [
    "## 5. ğŸ“ˆ Seasonal and Usage Pattern Analysis\n",
    "\n",
    "Let's explore how consumption varies by season and usage patterns - important factors for accurate predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze seasonal patterns\n",
    "seasonal_analysis = df.groupby(['season', 'appliance_type'])['daily_consumption_kwh'].mean().reset_index()\n",
    "seasonal_pivot = seasonal_analysis.pivot(index='appliance_type', columns='season', values='daily_consumption_kwh')\n",
    "\n",
    "# Create heatmap for seasonal consumption patterns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(seasonal_pivot, annot=True, cmap='YlOrRd', fmt='.2f', cbar_kws={'label': 'Daily Consumption (kWh)'})\n",
    "plt.title('ğŸŒ¡ï¸ Seasonal Consumption Patterns by Appliance Type', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Season', fontweight='bold')\n",
    "plt.ylabel('Appliance Type', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Usage pattern analysis\n",
    "usage_pattern_analysis = df.groupby(['usage_pattern', 'appliance_type'])['daily_consumption_kwh'].mean().reset_index()\n",
    "usage_pivot = usage_pattern_analysis.pivot(index='appliance_type', columns='usage_pattern', values='daily_consumption_kwh')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(usage_pivot, annot=True, cmap='Blues', fmt='.2f', cbar_kws={'label': 'Daily Consumption (kWh)'})\n",
    "plt.title('ğŸ“Š Usage Pattern Impact on Consumption by Appliance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Usage Pattern', fontweight='bold')\n",
    "plt.ylabel('Appliance Type', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸŒ¡ï¸ SEASONAL INSIGHTS:\")\n",
    "print(\"=\" * 25)\n",
    "for season in seasonal_pivot.columns:\n",
    "    max_appliance = seasonal_pivot[season].idxmax()\n",
    "    max_consumption = seasonal_pivot[season].max()\n",
    "    print(f\"ğŸ“… {season}: {max_appliance} consumes most ({max_consumption:.2f} kWh/day)\")\n",
    "\n",
    "print(\"\\nğŸ“Š USAGE PATTERN INSIGHTS:\")\n",
    "print(\"=\" * 30)\n",
    "for pattern in usage_pivot.columns:\n",
    "    avg_consumption = usage_pivot[pattern].mean()\n",
    "    print(f\"ğŸ”„ {pattern}: Average {avg_consumption:.2f} kWh/day across all appliances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bac863",
   "metadata": {},
   "source": [
    "## 6. ğŸ’¡ Key Insights Summary\n",
    "\n",
    "Let's summarize our findings and understand what this means for building our neural network model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ee628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Neural Network Data Exploration Summary\n",
    "\n",
    "print(\"ğŸ¯ NEURAL NETWORK DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset summary\n",
    "print(f\"\\nğŸ“Š Dataset Overview:\")\n",
    "print(f\"   Total samples: {df.shape[0]:,}\")\n",
    "print(f\"   Total features: {df.shape[1]}\")\n",
    "print(f\"   Target variable: daily_consumption\")\n",
    "\n",
    "# Feature analysis summary\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nğŸ”¢ Feature Analysis:\")\n",
    "print(f\"   Numeric features: {len(numeric_features)} (ready for neural network)\")\n",
    "print(f\"   Categorical features: {len(categorical_features)} (need encoding)\")\n",
    "\n",
    "# Data quality summary\n",
    "missing_count = df.isnull().sum().sum()\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "print(f\"\\nâœ… Data Quality:\")\n",
    "print(f\"   Missing values: {missing_count}\")\n",
    "print(f\"   Duplicate rows: {duplicate_count}\")\n",
    "print(f\"   Data completeness: {((len(df) - missing_count) / len(df)) * 100:.1f}%\")\n",
    "\n",
    "# Neural network readiness\n",
    "print(f\"\\nğŸ§  Neural Network Readiness:\")\n",
    "print(f\"   âœ… Dataset loaded and explored\")\n",
    "print(f\"   âœ… Target variable identified\")\n",
    "print(f\"   âœ… Feature correlations analyzed\")\n",
    "print(f\"   âœ… Data quality assessed\")\n",
    "print(f\"   âœ… Visualizations created\")\n",
    "\n",
    "print(f\"\\nğŸš€ Next Steps:\")\n",
    "print(f\"   1. â¡ï¸  Data Preprocessing (02_data_preprocessing.ipynb)\")\n",
    "print(f\"   2. ğŸ§  Neural Network Model Development (03_neural_network_model.ipynb)\")\n",
    "print(f\"   3. ğŸ“Š Model Evaluation (04_model_evaluation.ipynb)\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Data exploration completed successfully!\")\n",
    "print(f\"ğŸ“ˆ Ready to proceed with TensorFlow/Keras neural network development!\")\n",
    "\n",
    "# Save exploration insights for next notebook\n",
    "exploration_summary = {\n",
    "    'dataset_shape': df.shape,\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'target_variable': 'daily_consumption',\n",
    "    'data_quality': {\n",
    "        'missing_values': missing_count,\n",
    "        'duplicates': duplicate_count,\n",
    "        'completeness': ((len(df) - missing_count) / len(df)) * 100\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ’¾ Exploration summary saved for neural network pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
